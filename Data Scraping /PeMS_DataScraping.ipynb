{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PeMS_DataScraping.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZ+0dqvNni0T9+svGEQJfD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OTz6LdlkhlKb","colab_type":"code","colab":{}},"source":["import logging\n","import sys\n","import time\n","\n","from datetime import date, timedelta\n","from requests import session, ConnectionError\n","from numpy.random import random_integers\n","import configparser\n","from html.parser import HTMLParser"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ro3VE52ht-B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3f1faafd-22f3-469c-9c3a-1e19be8ce7d6","executionInfo":{"status":"ok","timestamp":1579718240351,"user_tz":480,"elapsed":3385,"user":{"displayName":"Rajasree Rajendran","photoUrl":"","userId":"08925108043915336138"}}},"source":["!pip install configparser"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: configparser in /usr/local/lib/python3.6/dist-packages (4.0.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cH32fADkrhzO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"6fcd78f1-9d1b-4448-e4a3-192e6b3e8833","executionInfo":{"status":"ok","timestamp":1579720542657,"user_tz":480,"elapsed":25782,"user":{"displayName":"Rajasree Rajendran","photoUrl":"","userId":"08925108043915336138"}}},"source":["import importlib.util\n","import sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIRarPVXiebA","colab_type":"code","colab":{}},"source":["class MyHTMLParser(HTMLParser):\n","    \"\"\"\n","    Parser to extract download links\n","    \"\"\"\n","    def __init__(self, dl_links=None):\n","        HTMLParser.__init__(self)\n","        if dl_links is None:\n","            self.dl_links = []\n","        else:\n","            self.dl_links = dl_links\n","            \n","    def handle_starttag(self, tag, attrs, key_string=\"download\"):\n","        if tag == 'a' and 'href' in [at[0] for at in attrs]:\n","            link = dict(attrs).get('href')\n","            #  Check if it is a download link\n","            if key_string in link:\n","                self.dl_links.append(dict(attrs).get('href'))\n","\n","            \n","\n","    def daterange(start_date, end_date):\n","        \"\"\"\n","          Tool for iterating over range of dates.\n","        \"\"\"\n","        for n in range(int ((end_date - start_date).days)):\n","          yield start_date + timedelta(n)\n","            \n","            \n","    def download_links(link_list, dt, out_path, url_base='http://pems.dot.ca.gov/'):\n","        \"\"\"\n","          Downloads the list of links from the PeMS clearinghouse. \n","    \n","          Args:\n","            link_list (list): List of links to files to download\n","            dt (dict): Data dict to be used in POST request. Includes\n","            username and password, which are spec\n","           Returns:\n","          int: Number of files downloaded\n","        \"\"\"\n","        with session() as c:\n","        #  POST the login request\n","          c.post(url_base, data=dt) \n","          for i,link in enumerate(link_list):\n","              ts = 10 #  Default time to sleep\n","              print (\"Iteration: \" + str(i))\n","              logging.info('initial time to sleep ' + str(ts))\n","              while True:\n","                  try: # Download with 10-second sleep time breaks\n","                      print ('Downloading file number: ' +str(i))\n","                      logging.info('try to download file: '  + str(i))\n","                      logging.info('time to sleep ' + str(ts))\n","                      # Make the request and download attached file\n","                      r = c.get(link)\n","                      fname = r.headers['content-disposition'].split('=')[-1]\n","                      with open(out_path+fname, 'wb') as fi: fi.write(r.content)\n","                      time.sleep(random_integers(ts,int(1.2*ts)))\n","                  except ConnectionError:\n","                      logging.warning('ConnectionError')\n","                      ts = ts*2\n","                      time.sleep(ts) #  Sleep and login again\n","                      c.post(url_base, data=dt) #, params=p)\n","                      continue\n","                  break\n","                \n","            \n","          if __name__ == \"__main__\":\n","            if len(sys.argv) < 2:\n","                print (\"ERROR: need to provide path to config file.\")\n","                exit\n","            config_path = sys.argv[1]\n","        \n","    # Load config constants\n","        from configparser import ConfigParser\n","        config = ConfigParser()\n","        config_path = '/content/drive/Shared drives/DATA298B/Data Scraping/config/example_config_clearinghouse.ini'\n","        config.read(config_path)\n","        html_path = config.get('Paths', 'html_file_path')\n","        log_path = config.get('Paths', 'log_file_path')\n","        out_path = config.get('Paths', 'out_dir_path')\n","        username = config.get('Creds', 'username')\n","        pwd = config.get('Creds', 'password')\n","            \n","    # Start logger\n","        logging.basicConfig(filename=log_path, level=logging.DEBUG)\n","\n","    # Parse HTML for download links\n","        logging.info('Parsing HTML to extract download links')\n","        with open(html_path, 'r') as fi:\n","          html_string = fi.read()\n","          parser = MyHTMLParser()\n","          parser.feed(html_string)\n","          \n","          # Download all the links\n","          dt = {\n","                  'action' : 'login',\n","                  'username' : username,\n","                  'password': pwd\n","          }\n","          download_links(parser.dl_links, dt, out_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfIK2ZLZjAQh","colab_type":"code","colab":{}},"source":["from configparser import ConfigParser\n","config = ConfigParser()\n","config_path = '/content/drive/Shared drives/DATA298B/Data Scraping/config/example_config_clearinghouse.ini'\n","config.read(config_path)\n","html_path = config.get('Paths', 'html_file_path')\n","log_path = config.get('Paths', 'log_file_path')\n","out_path = config.get('Paths', 'out_dir_path')\n","username = config.get('Creds', 'username')\n","pwd = config.get('Creds', 'password')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgVG57wi6UYD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}