import sys#sys.path.append('./src')import loggingimport boto3from configparser import ConfigParserimport pandas as pdimport threadingimport timefrom threading import Threadimport process_utils as utils#from omnisci_connector.omni_connect import OmnisciConnectfrom MetaData import MetaDatafrom TrafficData import TrafficDatafrom utils import locate_configfrom botocore.exceptions import ClientErrorimport os#############################INPUTS#############################table_name = "test_2"# initial parameters for reading in traffic datathreshold = 0.01interest_col = 'speed'grouper = 'station'batch_limit = 1file_ext = '.txt'thread_num = 1DEBUG = False################################################################"""def upload_file(file_name, bucket, object_name=None):    Upload a file to an S3 bucket    :param file_name: File to upload    :param bucket: Bucket to upload to    :param object_name: S3 object name. If not specified then file_name is used    :return: True if file was uploaded, else False        # If S3 object_name was not specified, use file_name    if object_name is None:        object_name = file_name    # Upload the file    s3_client = boto3.client('s3')    try:        response = s3_client.upload_file(file_name, bucket, object_name)    except ClientError as e:        logging.error(e)        return False    return True########"""def transform_and_load(config_path, paths, meta_path, thread,ind):    df_out =[]    df_batch =[]    try:        traffic_data =TrafficData(paths)        meta_data = MetaData(meta_path)        data = traffic_data.join_meta(meta_df=meta_data.df)        df_transformed = utils.apply_custom_transformations(df=data,interest_col=interest_col,threshold=threshold,grouper=grouper)        if not os.path.isfile(r'/Users/sravichandran/Desktop/Sindu/Traffic/Data Scraping /output/preprocessed_data_2015/preprocessed_2015.csv'):            df_transformed.to_csv (r'/Users/sravichandran/Desktop/Sindu/Traffic/Data Scraping /output/preprocessed_data_2015/preprocessed_2015.csv', index = None, header=True)        else:            df_transformed.to_csv(r'/Users/sravichandran/Desktop/Sindu/Traffic/Data Scraping /output/preprocessed_data_2015/preprocessed_2015.csv',mode='a',header=False)    except ValueError as ex:        print(ex)        print("could not process ", paths[0])        return        #connection.load_data(table_name=table_name,                         #df=df_transformed,                         #method='infer',                         #create='infer')    #connection.close_connection()def chunks(l, n):    """Yield successive n-sized chunks from l."""    for i in range(0, len(l), n):        yield l[i:i + n]if __name__ == "__main__":    config_path = locate_config(sys.argv)    # Configuration file reader    config = ConfigParser()    config.read(config_path, encoding='utf-8-sig')    print("Configuration file read.")    traffic_meta_path = config.get('Paths', 'meta_path')    csv_files = config.get('Paths', 'data_path')    print(csv_files)    file_paths = utils.get_file_names(csv_files, extension=file_ext)    output_path = config.get('Paths', 'out_dir_path')    print("Number of traffic files found: ", len(file_paths))    if DEBUG:        end = batch_limit    else:        end = len(file_paths)    for i in range(0, end, batch_limit):        files = file_paths[i:i + batch_limit]        threads = []        for j in range(thread_num):            n = batch_limit / thread_num            assert batch_limit % thread_num == 0            chunked_files = list(chunks(files, int(n)))            t = threading.Thread(target=transform_and_load, args=(config_path, chunked_files[j], traffic_meta_path,j,i))            threads.append(t)            t.start()        for t in threads:            t.join()# Upload to s3s3 = boto3.client('s3', aws_access_key_id='',                  aws_secret_access_key='')print("S3 upload")try:    s3.upload_file(r'/Users/sravichandran/Desktop/Sindu/Traffic/Data Scraping /output/preprocessed_data_2015/preprocessed_2015.csv', 'traffic--data', '2015_data.csv')    print("Upload Successful")except FileNotFoundError:    print("The file was not found")print("Exiting Main Thread")